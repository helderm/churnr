@Article{einstein2016,
  author = 	 {Einstein, Albert},
  title = 	 {On the Relative Value of Relatives},
  journal = 	 {J. Irreproducible Results},
  year = 	 {2016},
  volume = 	 {17},
  pages = 	 {4711--4721}
}

@InProceedings{heisenberg2015,
  author = 	 {Heisenberg, Werner and Dirac, Paul},
  title = 	 {To be or not to be},
  booktitle =    {Proceedings of the Uncertain Society Annual Meeting},
  year = 	 {2015},
  editor = 	 {Schr√∂dinger, Erwin},
  pages =	 {21--22}
}

 @article{Pudipeddi2014,
 author = {Pudipeddi, Jagat and Akoglu, Leman and Tong, Hanghang},
 file = {:home/helder/Downloads/p469-akoglu.pdf:pdf},
 isbn = {9781450327459},
 keywords = {a sites,churn prediction,feature extraction,q,user churn},
 pages = {469--474},
 title = {{User Churn in Focused Question Answering Sites: Characterizations and Prediction}},
 year = {2014}
 }

@article{Wangperawong2016,
 abstract = {Customer temporal behavioral data was represented as images in order to perform churn prediction by leveraging deep learning architectures prominent in image classification. Supervised  learning was performed on labeled data of over 6 million customers using deep convolutional neural networks, which achieved an AUC of 0.743 on the test dataset using no more than 12 temporal        features for each customer. Unsupervised learning was conducted using autoencoders to better understand the reasons for customer churn. Images that maximally activate the hidden units of an         autoencoder trained with churned customers reveal ample opportunities for action to be taken to prevent churn among strong data, no voice users.},
 archivePrefix = {arXiv},
 arxivId = {1604.05377},
 author = {Wangperawong, Artit and Brun, Cyrille and Laudy, Dr. Olav and Pavasuthipaisit, Rujikorn},
 eprint = {1604.05377},
 keywords = {1,2,3,big data,churn and take,churn prediction,cnns,deep learning,deep learning by convolutional,has demonstrated superior performance,images,in many image processing,in order to        leverage,it,machine learning,neural networks,pro-active measures to prevent,specifically,such advances to predict,tasks,telecommunications,we construct a 2-,we represent customers as},
 pages = {1--6},
 title = {{Churn analysis using deep convolutional neural networks and autoencoders}},
 year = {2016}
 }

 @article{Tax2016,
 abstract = {Predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof. Existing methods in this space are  tailor-made for specific prediction tasks. Moreover, their relative accuracy is highly sensitive to the dataset at hand, thus requiring users to engage in trial-and-error and tuning when applying   them in a specific setting. This paper investigates Long Short-Term Memory (LSTM) neural networks as an approach to build consistently accurate models for a wide range of predictive process         monitoring tasks. First, we show that LSTMs outperform existing techniques to predict the next event of a running case and its timestamp. Next, we show how to use models for predicting the next     task in order to predict the full continuation of a running case. Finally, we apply the same approach to predict the remaining time, and show that this approach outperforms existing tailor-made     methods.},
 archivePrefix = {arXiv},
 arxivId = {1612.02130},
 author = {Tax, Niek and Verenich, Ilya and {La Rosa}, Marcello and Dumas, Marlon},
 eprint = {1612.02130},
 title = {{Predictive Business Process Monitoring with LSTM Neural Networks}},
 url = {http://arxiv.org/abs/1612.02130},
 year = {2016}
 }